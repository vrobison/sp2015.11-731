#!/usr/bin/env python
# -*- coding: utf-8 -*-

import argparse # optparse is deprecated
from itertools import islice # slicing for iterators
from nltk.corpus import wordnet as wn
from nltk.stem.porter import PorterStemmer
import nltk
import sys
import unicodedata
from unidecode import unidecode

# DRY

def exact_matches(h, ref):
	ct = 0
	rem = []
	for n,w in enumerate(h):
		if w in ref:
			ct +=1
		else: rem.append(w)
	return (ct,rem) 
    # or sum(w in ref for w in f) # cast bool -> int
    # or sum(map(ref.__contains__, h)) # ugly!

def Hmean(a,p,r):
	if p == 0 and r == 0:
		return 0
	else: return 2*p*r/(a*p+(1-a)*r)

def ngrammer(h):
	ngrams = []
	for n in range(len(h)-2):
		ngrams.append(h[n] + ' '+h[n+1])
	for n in range(len(h)-3):
		ngrams.append(h[n] + ' '+h[n+1]+' '+h[n+2])
	return ngrams

def main():
	parser = argparse.ArgumentParser(description='Evaluate translation hypotheses.')
    # PEP8: use ' and not " for strings
	parser.add_argument('-i', '--input', default='data/train-test.hyp1-hyp2-ref',
            help='input file (default data/train-test.hyp1-hyp2-ref)')
	parser.add_argument('-n', '--num_sentences', default=None, type=int,
            help='Number of hypothesis pairs to evaluate')
	parser.add_argument('-a', '--alpha', default=0.5, type=float,
            help='recall vs. precision tradeoff')
	parser.add_argument('-d','--delta',default=.5,type=float,
			help='weight of content vs function words')
	parser.add_argument('-s','--syndisc',default=1,type=float,
			help='value to discount synonym matches by')
    # note that if x == [1, 2, 3], then x[:None] == x[:] == x (copy); no need for sys.maxint
	opts = parser.parse_args()
    # we create a generator and avoid loading all sentences into a list
	def sentences():
		with open(opts.input) as f:
			for pair in f:
				yield [sentence.strip().split() for sentence in pair.split(' ||| ')]
    # note: the -n option does not work in the original code
	funcWords =set(['it','a','an','the','to','be','will','is'])
	for h1, h2, ref, h1stem, h2stem, refstem in islice(sentences(), opts.num_sentences):
		
		h1N = ngrammer(h1)
		h2N = ngrammer(h2)
		rsetN = ngrammer(ref)

		#bi/trigrams of stemmed words
		h1Nstem = ngrammer(h1stem)
		h2Nstem = ngrammer(h2stem)
		rsetNstem = set(ngrammer(refstem))
		
		#bi/trigrams on stemmed synonyms
		#h1synNstem = ngrammer

		rset = set(ref)
		rsetstem = set(refstem)
		#print ref2
		
		rsetN = set(rsetN)
		

		h1_exact_ctN,h1_remN = exact_matches(h1N,rsetN)
		h2_exact_ctN,h2_remN = exact_matches(h2N, rsetN)
		h1_exact_ct,h1_rem = exact_matches(h1, rset)
		h2_exact_ct,h2_rem = exact_matches(h2, rset)
		
		h1_stem_ctN,h1_stem_remN = exact_matches(h1Nstem, rsetNstem)
		h2_stem_ctN,h2_stem_remN = exact_matches(h2Nstem, rsetNstem)
		h1_stem_ct,h1_stem_rem = exact_matches(h1stem, rsetstem)
		h2_stem_ct,h2_stem_rem = exact_matches(h2stem, rsetstem)
		

		h1_match = h1_exact_ctN + h1_exact_ct + opts.delta*(h1_stem_ctN+h1_stem_ct-(h1_exact_ctN + h1_exact_ct))
		h2_match = h2_exact_ctN + h2_exact_ct + opts.delta*(h2_stem_ctN+h2_stem_ct-(h1_exact_ctN + h1_exact_ct))
		p1 = h1_match/float(len(h1))
		p2 = h2_match/float(len(h2))
		r1 = h1_match/float(len(ref))
		r2 = h2_match/float(len(ref))
		a = opts.alpha
		M1 = Hmean(a,p1,r1)
		M2 = Hmean(a,p2,r2)
		H=(-1 if M1 > M2 else # \begin{cases}
			(0 if M1 == M2
				else 1)) # \end{cases}
		print H
# convention to allow import of this file as a module
if __name__ == '__main__':
    main()
