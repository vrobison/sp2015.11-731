#!/usr/bin/env python
import argparse
import sys
import models
import heapq
import pickle
from collections import namedtuple

parser = argparse.ArgumentParser(description='Simple phrase based decoder.')
parser.add_argument('-i', '--input', dest='input', default='data/input', help='File containing sentences to translate (default=data/input)')
parser.add_argument('-t', '--translation-model', dest='tm', default='data/tm', help='File containing translation model (default=data/tm)')
parser.add_argument('-s', '--stack-size', dest='s', default=100, type=int, help='Maximum stack size (default=1)')
parser.add_argument('-n', '--num_sentences', dest='num_sents', default=sys.maxint, type=int, help='Number of sentences to decode (default=no limit)')
parser.add_argument('-l', '--language-model', dest='lm', default='data/lm', help='File containing ARPA-format language model (default=data/lm)')
parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False, help='Verbose mode (default=off)')
opts = parser.parse_args()

tm = models.TM(opts.tm, sys.maxint)
lm = models.LM(opts.lm)
sys.stderr.write('Decoding %s...\n' % (opts.input,))
input_sents = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]
with open('trigram_spanish.pickle', 'r') as fd:
    tagger = pickle.load(fd)
def tag_and_swap(src):
	
	swapped = []
	tagged = list(tagger.tag(src))
	
	everchanged = False
	for n in range(len(tagged)):
		changed = False
		#print str(tagged[n][1])[:1]
		if n < len(tagged)-1:
			#sys.stderr.write(str(tagged[n])+' '+str(tagged[n+1])+'\n')
			if (str(tagged[n][1])[0] == 'n' or tagged[n][1]==None) and str(tagged[n+1][1])[0] == 'a':
				#print '\n',tagged[n][0], tagged[n+1][0],'\n'
				#adj = tagged[n+1][0]
				#noun = tagged[n][0]
				#swapped.append(adj)
				#swapped.append(noun)
				#n += 2
				#everchanged = True
				tagged = tagged[:n]+tagged[n+1:n+2]+tagged[n:n+1]+tagged[n+2:]
			else:
				other = tagged[n][0]
		
			swapped.append(tagged[n][0])
	swapped.append(tagged[-1][0])
	#if (' '.join(src) != ' '.join(swapped)):
	#	sys.stderr.write('---------'+' '.join(src)+'\n')
	#	sys.stderr.write('>>>>>>>>>'+' '.join(swapped)+'\n')
		
	
	#if everchanged == True: print src,'\n' ,tagged,'\n',swapped
	return tuple(swapped)	

def maxTM(source):
	max_t = -sys.maxint - 1
	if source not in tm: return max_t
	for phrase in tm[source]:
		score = phrase.logprob
		lm_state = ()
		for word in phrase.english.split():
			(lm_state, word_logprob) = lm.score(lm_state, word)
			score += word_logprob
		if score > max_t: max_t = score
	return max_t

def futureCost(f):
	costTable = [[0 for _ in f] for _ in f]
	for i in range(len(f)):
		for j in range(len(f)-i):
			mtm = maxTM(f[j:j+1+i])
			if i == 0:
				costTable[j][j] = mtm #can't break down one word
			else:
				maxCost = mtm
				for seam in range(j,j+i):
					comboCost = costTable[j][seam] + costTable[seam+1][j+i] #get bigger phrase costs as sum of smaller components
					if comboCost > maxCost: maxCost = comboCost #splitting a better deal 
				costTable[j][j+i] = maxCost
	return costTable
#print cost
hypothesis = namedtuple('hypothesis', 'logprob, lm_state, predecessor, phrase, fcost')
def getwinner(f):
	# The following code implements a DP monotone decoding
	# algorithm (one that doesn't permute the target phrases).
	# Hence all hypotheses in stacks[i] represent translations of
	# the first i words of the input sentence.
	# HINT: Generalize this so that stacks[i] contains translations
	# of any i words (remember to keep track of which words those
	# are, and to estimate future costs)
	fcost = futureCost(f)
	initial_hypothesis = hypothesis(0.0, lm.begin(), None, None, fcost[0][len(f)-1])
	uncovered = tuple(0 for _ in f)
	stacks = [{} for _ in f] + [{}]
	stacks[0][uncovered] = initial_hypothesis
	for m, stack in enumerate(stacks[:-1]):
		# extend the top s hypotheses in the current stack
		for cov, h in heapq.nlargest(opts.s, stack.iteritems(), key=lambda (c, h): h.logprob + h.fcost): # prune
			for i in range(min(len(cov), m+8)):#reordering limit
				if cov[i] == 1: continue
				for j in xrange(i+1, len(f)+1):
					if sum(cov[i:j]) > 0: continue
					if f[i:j] in tm:
						covered = [0 for _ in f]
						gapStart = 0
						predCost = 0.0
						for k in range(len(f)):
							if cov[k]==1 or (k >= i and k < j):
								covered[k] = 1
								if gapStart < k: predCost += fcost[gapStart][k-1]
								gapStart = k + 1
						if gapStart < len(f): predCost += fcost[gapStart][len(f)-1]
						covered = tuple(covered)
						for phrase in tm[f[i:j]]:
							logprob = h.logprob + phrase.logprob
							lm_state = h.lm_state
							for word in phrase.english.split():
								(lm_state, word_logprob) = lm.score(lm_state, word)
								logprob += word_logprob
							logprob += lm.end(lm_state) if j == len(f) else 0.0
							new_hypothesis = hypothesis(logprob, lm_state, h, phrase, predCost)
							if covered not in stacks[sum(covered)] or stacks[sum(covered)][covered].logprob < logprob: # second case is recombination
								stacks[sum(covered)][covered] = new_hypothesis
# find best translation by looking at the best scoring hypothesis
# on the last stack
	winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
	return winner
for f in input_sents:
	winner = getwinner(f)
	f2 = tag_and_swap(f)
	if f != f2:
		winner2 = getwinner(f2)
		if winner2.logprob > winner.logprob: winner = winner2
	
	def extract_english_recursive(h):
		return '' if h.predecessor is None else '%s%s ' % (extract_english_recursive(h.predecessor), h.phrase.english)
	print extract_english_recursive(winner)

	if opts.verbose:
		def extract_tm_logprob(h):
			return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
		tm_logprob = extract_tm_logprob(winner)			
		sys.stderr.write('LM = %f, TM = %f, Total = %f\n' %
			(winner.logprob - tm_logprob, tm_logprob, winner.logprob))
